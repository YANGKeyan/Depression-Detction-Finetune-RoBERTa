{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f4ff7-2f17-4333-9733-453742328f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning\troberta\troberta-large\tv1\n",
      "Some weights of the model checkpoint at models/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at models/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at models/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at models/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:585: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      " 12%|█████▋                                       | 1/8 [00:00<00:01,  5.98it/s]\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1 of 10:   0%|                                     | 0/10 [00:00<?, ?it/s]\n",
      "Running Epoch 0 of 10:   0%|                              | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    0.8062:   0%|               | 0/8 [00:00<?, ?it/s]\u001b[A/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "\n",
      "Epochs 0/10. Running Loss:    0.8062:  12%|▉      | 1/8 [00:00<00:01,  4.43it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.4170:  12%|▉      | 1/8 [00:00<00:01,  4.43it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.4170:  25%|█▊     | 2/8 [00:00<00:01,  5.16it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.2500:  25%|█▊     | 2/8 [00:00<00:01,  5.16it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.2500:  38%|██▋    | 3/8 [00:00<00:01,  4.70it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    0.7861:  38%|██▋    | 3/8 [00:00<00:01,  4.70it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    0.7861:  50%|███▌   | 4/8 [00:00<00:00,  4.63it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.3721:  50%|███▌   | 4/8 [00:00<00:00,  4.63it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.3721:  62%|████▍  | 5/8 [00:01<00:00,  4.76it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.2871:  62%|████▍  | 5/8 [00:01<00:00,  4.76it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.2871:  75%|█████▎ | 6/8 [00:01<00:00,  4.88it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.1055:  75%|█████▎ | 6/8 [00:01<00:00,  4.88it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.1055:  88%|██████▏| 7/8 [00:01<00:00,  4.99it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.4268:  88%|██████▏| 7/8 [00:01<00:00,  4.99it/s]\u001b[A\n",
      "Epochs 0/10. Running Loss:    1.4268: 100%|███████| 8/8 [00:01<00:00,  4.92it/s]\u001b[A\n",
      "/root/miniconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:1426: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:00,  5.19it/s]\u001b[A\n",
      "Epoch 2 of 10:  10%|██▉                          | 1/10 [00:30<04:30, 30.08s/it]\n",
      "Running Epoch 1 of 10:   0%|                              | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.1846:   0%|               | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.1846:  12%|▉      | 1/8 [00:00<00:01,  4.54it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.3291:  12%|▉      | 1/8 [00:00<00:01,  4.54it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.3291:  25%|█▊     | 2/8 [00:00<00:01,  4.93it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    0.9995:  25%|█▊     | 2/8 [00:00<00:01,  4.93it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    0.9995:  38%|██▋    | 3/8 [00:00<00:00,  5.08it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    0.9316:  38%|██▋    | 3/8 [00:00<00:00,  5.08it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    0.9316:  50%|███▌   | 4/8 [00:00<00:00,  4.89it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.4912:  50%|███▌   | 4/8 [00:00<00:00,  4.89it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.4912:  62%|████▍  | 5/8 [00:01<00:00,  4.83it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.0283:  62%|████▍  | 5/8 [00:01<00:00,  4.83it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.0283:  75%|█████▎ | 6/8 [00:01<00:00,  4.83it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.2158:  75%|█████▎ | 6/8 [00:01<00:00,  4.83it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.2158:  88%|██████▏| 7/8 [00:01<00:00,  4.78it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.1289:  88%|██████▏| 7/8 [00:01<00:00,  4.78it/s]\u001b[A\n",
      "Epochs 1/10. Running Loss:    1.1289: 100%|███████| 8/8 [00:01<00:00,  4.80it/s]\u001b[A\n",
      "/root/miniconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:1426: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:01,  4.33it/s]\u001b[A\n",
      "Epoch 3 of 10:  20%|█████▊                       | 2/10 [01:36<06:50, 51.26s/it]\n",
      "Running Epoch 2 of 10:   0%|                              | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.5698:   0%|               | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.5698:  12%|▉      | 1/8 [00:00<00:01,  6.60it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.9214:  12%|▉      | 1/8 [00:00<00:01,  6.60it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.9214:  25%|█▊     | 2/8 [00:00<00:01,  5.91it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    1.0693:  25%|█▊     | 2/8 [00:00<00:01,  5.91it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    1.0693:  38%|██▋    | 3/8 [00:00<00:00,  5.59it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.8047:  38%|██▋    | 3/8 [00:00<00:00,  5.59it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.8047:  50%|███▌   | 4/8 [00:00<00:00,  5.44it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    1.3662:  50%|███▌   | 4/8 [00:00<00:00,  5.44it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    1.3662:  62%|████▍  | 5/8 [00:00<00:00,  5.38it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.8018:  62%|████▍  | 5/8 [00:00<00:00,  5.38it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.8018:  75%|█████▎ | 6/8 [00:01<00:00,  5.46it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    1.2666:  75%|█████▎ | 6/8 [00:01<00:00,  5.46it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    1.2666:  88%|██████▏| 7/8 [00:01<00:00,  5.48it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.9790:  88%|██████▏| 7/8 [00:01<00:00,  5.48it/s]\u001b[A\n",
      "Epochs 2/10. Running Loss:    0.9790: 100%|███████| 8/8 [00:01<00:00,  5.54it/s]\u001b[A\n",
      "/root/miniconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:1426: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:01,  4.64it/s]\u001b[A\n",
      "Epoch 4 of 10:  30%|████████▋                    | 3/10 [01:56<04:18, 36.96s/it]\n",
      "Running Epoch 3 of 10:   0%|                              | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.9014:   0%|               | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.9014:  12%|▉      | 1/8 [00:00<00:01,  5.08it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.0459:  12%|▉      | 1/8 [00:00<00:01,  5.08it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.0459:  25%|█▊     | 2/8 [00:00<00:01,  5.40it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.7832:  25%|█▊     | 2/8 [00:00<00:01,  5.40it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.7832:  38%|██▋    | 3/8 [00:00<00:00,  5.27it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.3262:  38%|██▋    | 3/8 [00:00<00:00,  5.27it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.3262:  50%|███▌   | 4/8 [00:00<00:00,  5.22it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.1074:  50%|███▌   | 4/8 [00:00<00:00,  5.22it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.1074:  62%|████▍  | 5/8 [00:00<00:00,  5.14it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.8311:  62%|████▍  | 5/8 [00:01<00:00,  5.14it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.8311:  75%|█████▎ | 6/8 [00:01<00:00,  5.09it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.2383:  75%|█████▎ | 6/8 [00:01<00:00,  5.09it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    1.2383:  88%|██████▏| 7/8 [00:01<00:00,  5.16it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.9595:  88%|██████▏| 7/8 [00:01<00:00,  5.16it/s]\u001b[A\n",
      "Epochs 3/10. Running Loss:    0.9595: 100%|███████| 8/8 [00:01<00:00,  5.22it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --models RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590afc7-36c5-423a-b384-37ba393e8a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
